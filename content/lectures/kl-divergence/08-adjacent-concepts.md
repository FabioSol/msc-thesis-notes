---
title: "Adjacent Concepts"
date: 2026-02-04T18:28:37-06:00
draft: false
type: "document"
description: "Adjacent concepts to understand Kullbackâ€“Leibler divergence"
showDescription: false
weight: 8
---

## Entropy
Entropy is a measure of the uncertainty or randomness in a random variable or probability distribution. It is defined as:
$$H(P) = -\sum_{x \in \mathcal{X}} P(x) \log P(x)$$

where $P$ is the probability distribution of the random variable. Entropy can be thought of as the average amount of information produced by a stochastic source of data.

## Suprisal

Suprisal, also known as self-information, is a measure of the information content of an event. It is defined as:
$$I(x) = -\log P(x)$$

where $P(x)$ is the probability of the event $x$. The suprisal of an event is higher when the event is less likely to occur, and it is lower when the event is more likely to occur.

## Measure (Measure Theory)

In measure theory, a measure is a systematic way to assign a number to subsets of a given set, which can be thought of as a generalized notion of length, area, or volume. A measure must satisfy certain properties, such as non-negativity, null empty set, and countable additivity. In the context of probability theory, a probability measure assigns probabilities to events in a sample space.

> It is a rule that tells you how big a set is. 

